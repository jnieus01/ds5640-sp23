---
title: "Homework 1"
author: "Jordan Nieusma"
date: "January 19, 2023"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Using the RMarkdown/knitr/github mechanism, implement the following tasks by extending the example R script mixture-data-lin-knn.R:
[x] Paste the code from the mixture-data-lin-knn.R file into the homework template Knitr document.
[x] Read the help file for R's built-in linear regression function lm
[x] Re-write the functions fit_lc and predict_lc using lm, and the associated predict method for lm objects.
[x] Consider making the linear classifier more flexible, by adding squared terms for x1 and x2 to the linear model
[x] Describe how this more flexible model affects the bias-variance tradeoff

```{r libraries}
library("class")
library("dplyr")
```

```{r load-data}
## load binary classification example data from author website 
## 'ElemStatLearn' package no longer available
load(url('https://web.stanford.edu/~hastie/ElemStatLearn/datasets/ESL.mixture.rda'))
dat <- ESL.mixture
```

```{r}
# plot data points and decision boundary
plot_mix_data <- function(dat, datboot=NULL) {
  if(!is.null(datboot)) {
    dat$x <- datboot$x
    dat$y <- datboot$y
  }
  plot(dat$x[,1], dat$x[,2],
       col=ifelse(dat$y==0, 'blue', 'orange'),
       pch=20,
       xlab=expression(x[1]),
       ylab=expression(x[2]))
  ## draw Bayes (True) classification boundary
  prob <- matrix(dat$prob, length(dat$px1), length(dat$px2)) # creates 69 x 99 matrix
  cont <- contourLines(dat$px1, dat$px2, prob, levels=0.5) 
  rslt <- sapply(cont, lines, col='purple')
}

plot_mix_data(dat)
```

## Re-write the functions fit_lc and predict_lc using lm and the associated predict method for lm objects:

```{r}
# recall that x = input, y = quantitative output
## fit linear classifier

# Modified fit_lc:
fit_lc <- function(y, x1, x2) {
  beta <- lm(y ~ x1 + x2)
  beta
  
  # original code: 
  # x <- cbind(1, x)   # adds column of 1s to dat$x
  # beta <- drop(solve(t(x)%*%x)%*%t(x)%*%y)   # dot product of (x*x)-1 * (x*y) 
}

# Modified predict_lc:
## make predictions from linear classifier
predict_lc <- function(x, beta) {
  x <- data.frame(1, x)
  predict.lm(object = beta, newdata = x)
  # original code:
  # cbind(1, x) %*% beta
}

## fit model to mixture data and make predictions
lc_beta <- fit_lc(dat$y, dat$x[,1], dat$x[,2])
lc_pred <- predict_lc(dat$xnew, lc_beta)

## reshape predictions as a matrix
lc_pred <- matrix(lc_pred, length(dat$px1), length(dat$px2))
contour(lc_pred,
      xlab=expression(x[1]),
      ylab=expression(x[2]))


## find the contours in 2D space such that lc_pred == 0.5
lc_cont <- contourLines(dat$px1, dat$px2, lc_pred, levels=0.5)

## plot data and decision surface
plot_mix_data(dat)
sapply(lc_cont, lines)
```

## Consider making the linear classifier more flexible by adding squared terms for x1 and x2 to the linear model:

```{r}
# Modified fit_lc to include squared x1 and x2:
fit_lc <- function(y, x1, x2) {
  beta <- lm(y ~ x1 + x2 + I(x1^2) + I(x2^2)) 
  beta
  
  # original code: 
  # x <- cbind(1, x)   # adds column of 1s to dat$x
  # beta <- drop(solve(t(x)%*%x)%*%t(x)%*%y)   # dot product of (x*x)-1 * (x*y) 
}

# Modified predict_lc:
## make predictions from linear classifier
predict_lc <- function(x, beta) {
  x <- data.frame(1, x)
  predict.lm(object = beta, newdata = x)
  # original code:
  # cbind(1, x) %*% beta
}

## fit model to mixture data and make predictions
lc_beta <- fit_lc(dat$y, dat$x[,1], dat$x[,2])
lc_pred <- predict_lc(dat$xnew, lc_beta)

## reshape predictions as a matrix
lc_pred <- matrix(lc_pred, length(dat$px1), length(dat$px2))
contour(lc_pred,
      xlab=expression(x[1]),
      ylab=expression(x[2]))


## find the contours in 2D space such that lc_pred == 0.5
lc_cont <- contourLines(dat$px1, dat$px2, lc_pred, levels=0.5)

## plot data and decision surface
plot_mix_data(dat)
sapply(lc_cont, lines)
```

## Describe how this more flexible model affects the bias-variance tradeoff:

Variance refers to the amount by which the predicted values would change if estimated using a different training data set. Ideally, these estimates should not have variance between training data sets; a method with high variance means that small changes in the training data can yield large changes in the predicted values.

Bias refers to the error that is introduced approximating a real-life problem as a statistical model, since a statistical model inherently is a simplification of the real-life problem.

In general, the higher the flexibility of a model, the more strongly variance and bias are inversely related (as variance increases, bias will decrease and vice versa).  


```{r}
## fit knn classifier
## use 5-NN to estimate probability of class assignment
knn_fit <- knn(train=dat$x, test=dat$xnew, cl=dat$y, k=5, prob=TRUE)
knn_pred <- attr(knn_fit, 'prob')
knn_pred <- ifelse(knn_fit == 1, knn_pred, 1-knn_pred)

## reshape predictions as a matrix
knn_pred <- matrix(knn_pred, length(dat$px1), length(dat$px2))
contour(knn_pred,
        xlab=expression(x[1]),
        ylab=expression(x[2]),
        levels=c(0.25, 0.5, 0.75))


## find the contours in 2D space such that knn_pred == 0.5
knn_cont <- contourLines(dat$px1, dat$px2, knn_pred, levels=0.5)

## plot data and decision surface
plot_mix_data(dat)
sapply(knn_cont, lines)
```


```{r}
## do bootstrap to get a sense of variance in decision surface
resample <- function(dat) {
  idx <- sample(1:length(dat$y), replace = T)
  dat$y <- dat$y[idx]
  dat$x <- dat$x[idx,]
  return(dat)
}
  
## plot linear classifier for three bootstraps
par(mfrow=c(1,3))
for(b in 1:3) {
  datb <- resample(dat)
  ## fit model to mixture data and make predictions
  lc_beta <- fit_lc(datb$y, datb$x)
  lc_pred <- predict_lc(datb$xnew, lc_beta)
  
  ## reshape predictions as a matrix
  lc_pred <- matrix(lc_pred, length(datb$px1), length(datb$px2))

  ## find the contours in 2D space such that lc_pred == 0.5
  lc_cont <- contourLines(datb$px1, datb$px2, lc_pred, levels=0.5)
  
  ## plot data and decision surface
  plot_mix_data(dat, datb)
  sapply(lc_cont, lines)
}
```


```{r}
## plot 5-NN classifier for three bootstraps
par(mfrow=c(1,3))
for(b in 1:3) {
  datb <- resample(dat)
  
  knn_fit <- knn(train=datb$x, test=datb$xnew, cl=datb$y, k=5, prob=TRUE)
  knn_pred <- attr(knn_fit, 'prob')
  knn_pred <- ifelse(knn_fit == 1, knn_pred, 1-knn_pred)
  
  ## reshape predictions as a matrix
  knn_pred <- matrix(knn_pred, length(datb$px1), length(datb$px2))

  ## find the contours in 2D space such that knn_pred == 0.5
  knn_cont <- contourLines(datb$px1, datb$px2, knn_pred, levels=0.5)
  
  ## plot data and decision surface
  plot_mix_data(dat, datb)
  sapply(knn_cont, lines)
}
```


```{r}
## plot 20-NN classifier for three bootstraps
par(mfrow=c(1,3))
for(b in 1:3) {
  datb <- resample(dat)
  
  knn_fit <- knn(train=datb$x, test=datb$xnew, cl=datb$y, k=20, prob=TRUE)
  knn_pred <- attr(knn_fit, 'prob')
  knn_pred <- ifelse(knn_fit == 1, knn_pred, 1-knn_pred)
  
  ## reshape predictions as a matrix
  knn_pred <- matrix(knn_pred, length(datb$px1), length(datb$px2))
  
  ## find the contours in 2D space such that knn_pred == 0.5
  knn_cont <- contourLines(datb$px1, datb$px2, knn_pred, levels=0.5)
  
  ## plot data and decision surface
  plot_mix_data(dat, datb)
  sapply(knn_cont, lines)
}
```

